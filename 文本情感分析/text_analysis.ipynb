{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.WARN,stream=sys.stdout,format='%(levelname)s:%(message)s')\n",
    "\n",
    "VOCAB_SIZE = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编写LSTM模型代码\n",
    "class TinyLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes=2):\n",
    "        super(TinyLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        # 输入门i_t\n",
    "        self.W_xi = nn.Parameter(torch.randn(input_size, hidden_size))\n",
    "        self.W_hi = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
    "        self.b_i = nn.Parameter(torch.zeros(hidden_size))\n",
    "        # 遗忘门f_t\n",
    "        self.W_xf = nn.Parameter(torch.randn(input_size, hidden_size))\n",
    "        self.W_hf = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
    "        self.b_f = nn.Parameter(torch.zeros(hidden_size))\n",
    "        # 输出门o_t\n",
    "        self.W_xo = nn.Parameter(torch.randn(input_size, hidden_size))\n",
    "        self.W_ho = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
    "        self.b_o = nn.Parameter(torch.zeros(hidden_size))\n",
    "        # 候选单元g_t\n",
    "        self.W_xg = nn.Parameter(torch.randn(input_size, hidden_size))\n",
    "        self.W_hg = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
    "        self.b_g = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "        #   初始化参数\n",
    "        for param in self.parameters():\n",
    "            if param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, x, h_0=None, c_0=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        if h_0 is None:\n",
    "            h_0 = torch.zeros(batch_size,self.hidden_size, device=x.device)\n",
    "        if c_0 is None:\n",
    "            c_0 = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
    "        h_pre, c_pre = h_0, c_0\n",
    "        h_all = []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:,t, :] # 获取当前时间步的输入\n",
    "            i_t = torch.sigmoid(x_t @ self.W_xi + h_pre @ self.W_hi + self.b_i)\n",
    "            f_t = torch.sigmoid(x_t @ self.W_xf + h_pre @ self.W_hf + self.b_f)\n",
    "            o_t = torch.sigmoid(x_t @ self.W_xo + h_pre @ self.W_ho + self.b_o)\n",
    "            g_t = torch.tanh(x_t @ self.W_xg + h_pre @ self.W_hg + self.b_g)\n",
    "            c_t = f_t * c_pre + i_t * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            h_pre, c_pre = h_t, c_t\n",
    "            h_all.append(h_t.unsqueeze(1))\n",
    "        output = torch.cat(h_all, dim=1)  # (batch_size, seq_len, hidden_size)\n",
    "        return output, h_t, c_t\n",
    "    \n",
    "class TinyLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=VOCAB_SIZE, embed_dim=64, hidden_size=128, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = TinyLSTM(embed_dim, hidden_size)  # 你手写的LSTM\n",
    "        # self.lstm = nn.LSTM(embed_dim, hidden_size, batch_first=True)  # PyTorch内置LSTM\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)   # 如果用拼接，hidden_size*2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len)\n",
    "        emb = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        output, h_pre, c_pre = self.lstm(emb)\n",
    "        # 常用方式1：只用最后一个隐藏状态\n",
    "        logits = self.fc(h_pre)\n",
    "        # 常用方式2：池化\n",
    "        # pooled = output.mean(dim=1)\n",
    "        # logits = self.fc(pooled)\n",
    "        # 结合方式\n",
    "        # features = torch.cat([h_pre, pooled], dim=1)\n",
    "        # logits = self.fc(features)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编写GCNN模型代码\n",
    "class GCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes):\n",
    "        super(GCNN, self).__init__()\n",
    "\n",
    "        self.embedding_table = nn.Embedding(vocab_size, embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.embedding_table.weight)\n",
    "\n",
    "        self.conv_A_1 = nn.Conv1d(embedding_dim, 64, 15, stride=7)\n",
    "        self.conv_B_1 = nn.Conv1d(embedding_dim, 64, 15, stride=7)\n",
    "\n",
    "        self.conv_A_2 = nn.Conv1d(64, 64, 15, stride=7)\n",
    "        self.conv_B_2 = nn.Conv1d(64, 64, 15, stride=7)\n",
    "\n",
    "        self.output_linear1 = nn.Linear(64, 128)\n",
    "        self.output_linear2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, word_index):\n",
    "        # 定义GCN网络的算子操作流程，基于句子单词ID输入得到分类logits输出\n",
    "        \n",
    "        # 1.通过word_index得到word_embedding\n",
    "        # word_index shape:[batch_size, seq_len]\n",
    "        word_embedding = self.embedding_table(word_index)\n",
    "    \n",
    "        # word_embedding shape:[batch_size, seq_len, embedding_dim]\n",
    "        word_embedding = word_embedding.permute(0, 2, 1)\n",
    "        # 2.编写第一层1D门卷积模块\n",
    "        A = self.conv_A_1(word_embedding)\n",
    "        B = self.conv_B_1(word_embedding)\n",
    "        H = A * torch.sigmoid(B)\n",
    "\n",
    "        A = self.conv_A_2(H)\n",
    "        B = self.conv_B_2(H)\n",
    "        H = A * torch.sigmoid(B)\n",
    "\n",
    "        # 3.池化并经过全连接层\n",
    "        pool_output = torch.mean(H, dim=-1)\n",
    "        linear1_output = self.output_linear1(pool_output)\n",
    "        logits = self.output_linear2(linear1_output)\n",
    "        return logits\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size = VOCAB_SIZE, embed_dim=64,num_class=2):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        # \n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, token_index):\n",
    "        embedded = self.embedding(token_index)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "# build IMDB DataLoader\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "#@save\n",
    "d2l.DATA_HUB['aclImdb'] = (\n",
    "    'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz',\n",
    "    '01ada507287d82875905620988597833ad4e0903')\n",
    "\n",
    "data_dir = d2l.download_extract('aclImdb', 'aclImdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数目： 25000\n",
      "标签： 1 review: For a movie that gets no respect there sure are a lot of mem\n",
      "标签： 1 review: Bizarre horror movie filled with famous faces but stolen by \n",
      "标签： 1 review: A solid, if unremarkable film. Matthau, as Einstein, was won\n"
     ]
    }
   ],
   "source": [
    "#@save\n",
    "def read_imdb(data_dir, is_train):\n",
    "    \"\"\"读取IMDb评论数据集文本序列和标签\"\"\"\n",
    "    data, labels = [], []\n",
    "    for label in ('pos', 'neg'):\n",
    "        folder_name = os.path.join(data_dir, 'train' if is_train else 'test',\n",
    "                                   label)\n",
    "        for file in os.listdir(folder_name):\n",
    "            with open(os.path.join(folder_name, file), 'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n', '')\n",
    "                data.append(review)\n",
    "                labels.append(1 if label == 'pos' else 0)\n",
    "    return data, labels\n",
    "\n",
    "train_data = read_imdb(data_dir, is_train=True)\n",
    "print('训练集数目：', len(train_data[0]))\n",
    "for x, y in zip(train_data[0][:3], train_data[1][:3]):\n",
    "    print('标签：', y, 'review:', x[0:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def load_data_imdb(batch_size, num_steps=500):\n",
    "    \"\"\"返回数据迭代器和IMDb评论数据集的词表\"\"\"\n",
    "    data_dir = d2l.download_extract('aclImdb', 'aclImdb')\n",
    "    train_data = read_imdb(data_dir, True)\n",
    "    test_data = read_imdb(data_dir, False)\n",
    "    train_tokens = d2l.tokenize(train_data[0], token='word')\n",
    "    test_tokens = d2l.tokenize(test_data[0], token='word')\n",
    "    vocab = d2l.Vocab(train_tokens, min_freq=5)\n",
    "    train_features = torch.tensor([d2l.truncate_pad(\n",
    "        vocab[line], num_steps, vocab['<pad>']) for line in train_tokens])\n",
    "    test_features = torch.tensor([d2l.truncate_pad(\n",
    "        vocab[line], num_steps, vocab['<pad>']) for line in test_tokens])\n",
    "    train_iter = d2l.load_array((train_features, torch.tensor(train_data[1])),\n",
    "                                batch_size)\n",
    "    test_iter = d2l.load_array((test_features, torch.tensor(test_data[1])),\n",
    "                               batch_size,\n",
    "                               is_train=False)\n",
    "    return train_iter, test_iter, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data,test_data, model,optimizer,num_epoch,lr,\n",
    "          log_step_interval, save_step_interval,eval_step_interval,save_path,resume=\"\"):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    print(f'device:{device}')\n",
    "    start_epoch = 0\n",
    "    start_step = 0\n",
    "    if resume:\n",
    "        checkpoint = torch.load(resume, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        start_step = checkpoint['step']\n",
    "        logging.info(f\"Resuming from epoch {start_epoch}, step {start_step}\")\n",
    "    for epoch_index in range(start_epoch, num_epoch):\n",
    "        ema_loss = 0\n",
    "        num_batches = len(train_data)\n",
    "        for batch_index, (token_index,target ) in enumerate(train_data):\n",
    "            print(f'epoch_index:{epoch_index} batch_index:{batch_index}')\n",
    "            token_index = token_index.to(device)\n",
    "            target = target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            step = num_batches * epoch_index + batch_index + 1\n",
    "            logits = model(token_index)\n",
    "            bce_loss = F.binary_cross_entropy(torch.sigmoid(logits), F.one_hot(target, num_classes=2).to(torch.float32))\n",
    "            ema_loss = ema_loss * 0.9 + bce_loss * 0.1\n",
    "            bce_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            if step % log_step_interval == 0:\n",
    "                logging.info(f\"Epoch {epoch_index}, Step {step}, Loss: {ema_loss.item():.4f}\")\n",
    "            if epoch_index == num_epoch - 1 or step % save_step_interval == 0:\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch_index,\n",
    "                    'step': step,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict()\n",
    "                }\n",
    "                torch.save(checkpoint, os.path.join(save_path, f\"checkpoint_epoch_{epoch_index}_step_{step}.pt\"))\n",
    "                logging.info(f\"Checkpoint saved at epoch {epoch_index}, step {step}\")\n",
    "            if step % eval_step_interval == 0:\n",
    "                logging.info(f\"Evaluating at step {step}...\")\n",
    "                # Evaluate the model on the test data       \n",
    "                model.eval()\n",
    "                ema_loss_eval = 0\n",
    "                total_acc_account = 0\n",
    "                total_account = 0\n",
    "                for eval_batch_index,( eval_token_index,eval_target) in enumerate(test_data):\n",
    "                    total_account += eval_target.shape[0]\n",
    "                    eval_logits = model(eval_token_index)\n",
    "                    total_acc_account += (torch.argmax(eval_logits, dim=1) == eval_target).sum().item()\n",
    "                    eval_bce_loss = F.binary_cross_entropy(torch.sigmoid(eval_logits), F.one_hot(eval_target, num_classes=2).to(torch.float32))\n",
    "                    ema_loss_eval = ema_loss_eval * 0.9 + eval_bce_loss * 0.1\n",
    "                print(f\"Evaluation Loss: {ema_loss_eval.item():.4f}, Accuracy: {total_acc_account / total_account:.4f}\")\n",
    "                logging.info(f\"Evaluation Loss: {ema_loss_eval.item():.4f}, Accuracy: {total_acc_account / total_account:.4f}\")\n",
    "                model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data\n",
    "train_iter, test_iter, vocab = load_data_imdb(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "# model = TextClassificationModel(vocab_size=len(vocab), embed_dim=64, num_class=2)\n",
    "    # Evaluation Loss: 0.7483, Accuracy: 0.5403\n",
    "    # Evaluation Loss: 0.6055, Accuracy: 0.7680\n",
    "    # Evaluation Loss: 0.5356, Accuracy: 0.8023\n",
    "    # Evaluation Loss: 0.3907, Accuracy: 0.8227\n",
    "    # Evaluation Loss: 0.4177, Accuracy: 0.8483\n",
    "    # Evaluation Loss: 0.3278, Accuracy: 0.8553\n",
    "    # Evaluation Loss: 0.3279, Accuracy: 0.8632\n",
    "    # Evaluation Loss: 0.3185, Accuracy: 0.8691\n",
    "    # Evaluation Loss: 0.2957, Accuracy: 0.8717\n",
    "    # Evaluation Loss: 0.3145, Accuracy: 0.8761\n",
    "    # Evaluation Loss: 0.2690, Accuracy: 0.8767\n",
    "    # Evaluation Loss: 0.3092, Accuracy: 0.8798\n",
    "    # Evaluation Loss: 0.2458, Accuracy: 0.8780\n",
    "    # Evaluation Loss: 0.3051, Accuracy: 0.8811\n",
    "    # Evaluation Loss: 0.2951, Accuracy: 0.8830\n",
    "model = TinyLSTMClassifier(vocab_size=len(vocab), embed_dim=64, num_classes=2,hidden_size=128)\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "num_epoch = 20\n",
    "log_step_interval = 100\n",
    "save_step_interval = 500\n",
    "eval_step_interval = 500\n",
    "save_path = \"./checkpoints\"\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Train the model\n",
    "train(train_iter, test_iter, model, optimizer, num_epoch, 0.001,\n",
    "        log_step_interval, save_step_interval, eval_step_interval, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过保存的模型进行预测\n",
    "def predict(model, text, vocab, max_length=500):\n",
    "    model.eval()\n",
    "    tokens = d2l.tokenize([text], token='word')\n",
    "    # 将tokens转换为索引\n",
    "    indices = [vocab[token] for token in tokens[0]]\n",
    "    # 如果长度不足 max_length，则用 vocab['<pad>'] 填充\n",
    "    if len(indices) < max_length:\n",
    "        indices += [vocab['<pad>']] * (max_length - len(indices))\n",
    "    # 如果长度超过 max_length，则截断\n",
    "    indices = indices[:max_length]\n",
    "    indices = torch.tensor(indices).unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        logits = model(indices)\n",
    "        prediction = torch.argmax(logits, dim=1).item()\n",
    "    return \"Positive\" if prediction == 1 else \"Negative\"\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    model_path = \"./checkpoints/checkpoint_epoch_19_step_7500.pt\"\n",
    "    model = TextClassificationModel(vocab_size=len(vocab), embed_dim=64, num_class=2)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device)['model_state_dict'])\n",
    "    model.to(device)\n",
    "    # example_text = \"The movie is boring, and I didn't love it\"\n",
    "    example_text = \"someone said the movie was boring, but I really enjoyed it\"\n",
    "    prediction = predict(model, example_text, vocab)\n",
    "    print(f\"Prediction for '{example_text}': {prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (d2l)",
   "language": "python",
   "name": "d2l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
